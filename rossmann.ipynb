{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"static/img/rosslog.png\" align=\"left\">  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "e1e663f2-7799-45cf-ba9c-1b90636b5c6e",
    "_uuid": "82ab9b775fcea7f0a7fc33486264640f69b38bf0"
   },
   "source": [
    "# Rossmann Stores Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook covers basics of data cleaning, EDA, feature engineering, and data modelling. For more in-deph data analysis, please consider Kaggle Kernels: https://www.kaggle.com/c/rossmann-store-sales/kernels\n",
    "\n",
    "**IMPORTANT: Make sure you do not skip any cells. In that case you risk not being able to accomplish given tasks.** \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rossmann Data Cleaning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are provided with historical sales data for 1,115 Rossmann stores. The task is to forecast the \"Sales\" column for the test set. In this first part of the notebook we will walk you through basic steps that need to be done before start with baseline modelling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "dd08db0b-3c0e-4a12-b77d-348b430b33d7",
    "_uuid": "82658364a5412b45f3c1bdba11c0bc17a7a9105b"
   },
   "source": [
    "### Load libraries and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "acc6733e-b0f0-4d08-bd45-d50881091212",
    "_uuid": "511329117440698529039824b10aed63cf457722",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# NumPy for numerical computing\n",
    "import numpy as np\n",
    "\n",
    "# Pandas for DataFrames\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "from pandas import TimeGrouper\n",
    "\n",
    "# Matplotlib for visualization\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Display plots in the notebook\n",
    "%matplotlib inline \n",
    "\n",
    "# Seaborn for easier visualization\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "0a97eeb3-097f-45a3-8ebf-6309e6bcd0cc",
    "_uuid": "aadc7b21d040e576b32b8d0c5e5fd2dd5e22eedd",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load data from CSV\n",
    "train_dataset = pd.read_csv(\"static/data/train.csv\")\n",
    "stores = pd.read_csv(\"static/data/store.csv\")\n",
    "\n",
    "train_dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The train data set contains sales by day for each store with the following columns:\n",
    "\n",
    "- Store - a unique id number\n",
    "- DayOfWeek/Date - the day of the week (1-7) and date (YYYY-MM-DD) for a sales data point\n",
    "- Sales - the sales for a given day\n",
    "- Customers - the number of customers on a given day. This column is highly correlated with sales and is not present in the test set.\n",
    "- Open - Values: 0 = closed, 1 = open\n",
    "- Promo - indicates whether a store was running a sales promotion that day\n",
    "- StateHoliday - Values: a = public holiday, b = Easter holiday, c = Christmas, 0 = None\n",
    "- SchoolHoliday - indicates if a store was affected by the closure of public schools on that day"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# YOUR CODE GOES HERE\n",
    "# Print first five rows of the \"stores\" dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The stores data set contains additional columns about each store that does not vary by day:\n",
    "\n",
    "- StoreType - differentiates between 4 different store models: a, b, c, d\n",
    "- Assortment - describes the level of products available: a = basic, b = extra, c = extended\n",
    "- CompetitionDistance - distance in meters to the nearest competitor\n",
    "- CompetitionOpenSince[Month/Year] - month/year when the nearest competitor was opened\n",
    "- Promo2 - indicator for a recurring promotion: 0 = store not participating, 1 = participating\n",
    "- Promo2Since[Week/Year] - calendar week/year when the store started participating in Promo2\n",
    "- PromoInterval - describes the intervals when Promo2 is started. E.g. \"Feb,May,Aug,Nov\" means each round starts in those months of any given year for that store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# YOUR CODE GOES HERE\n",
    "# Chech out the shape of the datasets. \n",
    "# We have a lot of data observations to play around.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look and understand different data types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "99a7f662-57af-477a-a23c-3bfcf7ae5513",
    "_uuid": "7c499cc55c5007b8f58f5a5e1e5c64917286ff3e",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Column datatypes\n",
    "print(train_dataset.dtypes,'\\n')\n",
    "print(stores.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "453b25d1-f99c-4b13-8310-df4f736e5c6b",
    "_uuid": "62794228d5ba90099fdc5ce2f75cc31d89708258"
   },
   "source": [
    "### Drop unwanted data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The overall goals during the data cleaning phase is to fix any problems with the provided data sets, such as:\n",
    "\n",
    "- Fix inconsistent data\n",
    "- Impute missing data (There are many ways to impute data, but for the purpose of this assignment, we will simply drop missing data for convenience)\n",
    "- Convert categorical variables to one-hot encoded binary variables\n",
    "- Check for outlying values, and correct them if necessary\n",
    "\n",
    "We needed to fix these types of problems so that our prediction models could be fit as accurately as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "0b3e01b4-8d51-4029-8f29-40a89d95508b",
    "_uuid": "5c2634eec6e5b995d822ffbd09724dad67b003d5",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Check for duplicates\n",
    "train_dataset = train_dataset.drop_duplicates()\n",
    "stores = stores.drop_duplicates()\n",
    "\n",
    "# Print shape after removing duplicates\n",
    "train_dataset.shape, stores.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There were no duplicates. Now, let us check closed stores, stores with no customers, and stores where sales values equals zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "56e7d8b2-d9c0-4bba-9e38-b63fcb601814",
    "_uuid": "1d340477c6f43609eba155140b5f573ed21d6629",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Check how many records are for days when stores are closed\n",
    "print(\"The number of records for closed stores: \", train_dataset[train_dataset.Open == 0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though there are substantial number of records recorded when stores are closed, we are going to drop these observations because sales, our response variable, would be 0 for the days when the store is closed (there is no need for regression). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "56e7d8b2-d9c0-4bba-9e38-b63fcb601814",
    "_uuid": "1d340477c6f43609eba155140b5f573ed21d6629",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Drop observations with closed stores\n",
    "train_dataset = train_dataset[train_dataset.Open != 0]\n",
    "train_dataset.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Question 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# YOUR CODE GOES HERE\n",
    "# Print the number of stores where Customers=0\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Question 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# YOUR CODE GOES HERE\n",
    "# Print the number of stores where Sales=0\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us check for missing values in `train_dataset` and `stores`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(train_dataset.isnull().sum())\n",
    "print(stores.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop unuseful columns:\n",
    "- in `train_dataset`: `Open` is unuseful because it is consisted of all 1's (we dropped the observations with Open = 0 earlier).\n",
    "- in `stores`: `CompetitionOpenSinceMonth`, `CompetitionOpenSinceYear`, `Promo2SinceWeek`, `Promo2SinceYear`, `PromoInterval` because they have too many missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_dataset.drop(['Open'], axis=1, inplace=True)\n",
    "stores.drop(['CompetitionOpenSinceMonth', \n",
    "             'CompetitionOpenSinceYear', \n",
    "             'Promo2SinceWeek',\n",
    "             'Promo2SinceYear',\n",
    "             'PromoInterval'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop the 3 rows of `stores` where `CompetitionDistance` is missing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stores.dropna(inplace=True)\n",
    "stores.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Joining the \"Train\" and \"Store\" tables "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Question 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# YOUR CODE\n",
    "# inner join `stores_encode` and `train_dataset_encode` on `Store`, and save the resulting df as `df_combined`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that the combined df has no missing values and that it has the proper one-hot encoded values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_combined.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New Feature Engineering on df_combined\n",
    "- Extract month from `Date`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_combined['Month'] = pd.DatetimeIndex(df_combined['Date']).month.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "8339e1d5-cf5c-420f-bb40-3a92b3a5186e",
    "_uuid": "9c19f2bc0fca159a3cd0ba5f9683caf7ecfd5d4c"
   },
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To determine what type of models and predictors might work best for predicting sales, we studied which factors are causing the most variance in sales."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Variations based on Store Attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the biggest sources of variance in sales is based on the store number. In the graph below we show the average sales for the first ten stores in the data set. We can see that the average sales are quite different for each store and that there is no linear pattern. We do see stores with similar sales levels, suggesting the stores may fall into groups.\n",
    "\n",
    "The scatterplot below that shows the average sales for all the stores. We can again see that some stores have similar levels of sales, so it's likely we can group stores together when making sales predictions, which is something for which tree models are well suited.\n",
    "\n",
    "Each store number is effectively a category with its own particular sales level. When predicting future sales, we would like a single model that can make predictions for each store rather than having to fit a separate model for each store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_subset = df_combined[(df_combined['Store'] < 11)]\n",
    "sns.set(font_scale=2)\n",
    "\n",
    "# Plot store sales for stores 1 to 10\n",
    "fig, ax = plt.subplots(1,1)\n",
    "\n",
    "p3 = sns.barplot(x='Store', y='Sales', data=df_subset, ax=ax)\n",
    "ax.set(xlabel='Store Number')\n",
    "ax.set_ylabel('Average Sales')\n",
    "ax.set_title('Sales for ten stores')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Create a plot of average sales per store id\n",
    "avg_sales_per_store = df_combined[['Sales', 'Store']].groupby('Store').mean()\n",
    "avg_sales_per_store.reset_index().plot(kind='scatter', x='Store', y='Sales')\n",
    "plt.xlim(0,960)\n",
    "plt.title(\"Average Sales by Store Number\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data set has categorical variables for different store types and the assortment of products for sale at each store. We see that these categories are a source of variance in sales."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Impact of Number of Customers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After exploration of the train and test sets, we ascertained that only the training data contains the number of Customers feature. The visualization below shows the positive correlation between the number of customers and sales (and also outliers). This feature is highly correlated with sales, but it’s not available until after the sales occur (i.e. it’s not in the test set). But exploring this feature helps make a case for including a proxy for the Customers feature in our final predictors (see the Feature Engineering section).\n",
    "\n",
    "We also see that there’s added information with the behavior of customers per StoreType. We see that StoreType d (shown in green) that projects to the upper left quadrant. This means that for the same level of Sales, StoreType d requires fewer customers than StoreType b (blue). There’s less clarity of StoreType impact in the middle of the cone shaped scatter, but we can certainly see the impact of Customers and Sales conditional on the StoreType mostly in the outer regions of the visualization. Thus StoreType is a relevant feature, but as a function of Customers, as we can see that around 2000 to 3000 Customers there’s a break in the cluster pattern of the middle section of the cone. Therefore, further substantiation for us to include a proxy of the Customers feature in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sns.lmplot(x='Customers', y='Sales', data=df_combined, hue='StoreType',fit_reg=False,\n",
    "           scatter_kws={'alpha':0.1}, size=7, aspect=1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "0190bc75-61b4-489f-bffd-71d40eed41b6",
    "_uuid": "56594318e444d4391d5cfb516eb8c955f6d11ea5",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "g = sns.FacetGrid(df_combined, col=\"StoreType\", col_wrap=2, size=4, aspect=1.5)\n",
    "g.map(plt.scatter, \"Customers\", \"Sales\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process categorical columns\n",
    "\n",
    "Before we can start modeling, we need to convert categorical variables into one-hot encoded binary variables.\n",
    "In `df_combined`, one-hot encode the following:\n",
    "\n",
    "- `Store`\n",
    "- `StoreType`\n",
    "- `Assortment` \n",
    "- `StateHoliday` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that we have two zero values for `StateHoliday`: one integer and one string. Let us fix this and have one string zero value for all `StateHoliday` representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Display unique values of 'StateHoliday'\n",
    "df_combined.StateHoliday.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Question 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# YOUR CODE\n",
    "# Your result shoud look like this: \n",
    "### array(['0', 'a', 'b', 'c'], dtype=object)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Question 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# YOUR CODE\n",
    "# Transforme the variables into one-hot-encoding using the get_dummies() method\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "f8974e36-d25b-457c-baaa-88de595b76d8",
    "_uuid": "5b54e070f7cde4cf7b6f4e55da6b44289506aec7"
   },
   "source": [
    "## Rossmann Data Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to the regressive predictive nature of The Rossmann Project, we decided to approach our model methodology with a Linear Regression base model with feature selection. Note: you can also consider Ridge regression to evaluate the effects of regularization on the predictive performance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note: The time dimension has a tremendous impact in this project. The Kaggle competition consisted in predicted the next 6 weeks (or 42 days), our splitting lead to have a testing set very large of over 900 days, explaining the fast degradation of the explained variance. These issues are the same the world of finance and economics are facing. For example the usage of an AR(n) to predict future outcomes quickly converge to a stationary states. In a real world, one would adjust the model based on the observed error. This is the spirit of tools such as the 'Kalman Filter' that are usually implemented in these domains.* \n",
    "\n",
    "*One other potential approach would consist in implementing more complicated model that naturally combine time series and decision tree, such as ART (Autoregressive Trees). The proposed method are outside of the scope of this course and thus we will focus on some other tehniques here.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear regression is an intuitive first baseline model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_combined.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keep features for modeling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "store_feats = [c for c in df_combined if \"Store_\" in c] # columns related to one-hot encoded `Store`\n",
    "store_type_feats = [c for c in df_combined if \"StoreType_\" in c] # columns related to one-hot encoded `StoreType_`\n",
    "assortment_feats = [c for c in df_combined if \"Assortment_\" in c] # columns related to one-hot encoded `Assortment_`\n",
    "state_holiday_feats = [c for c in df_combined if \"StateHoliday_\" in c] # columns related to one-hot encoded `StateHoliday_`\n",
    "extra_feats = ['CompetitionDistance', 'Promo2', 'Promo',\n",
    "            'DayOfWeek', 'Month', \n",
    "            'SchoolHoliday']\n",
    "features = store_feats + store_type_feats + assortment_feats + state_holiday_feats + extra_feats\n",
    "\n",
    "target = 'Sales'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear Regression\n",
    "\n",
    "This takes a bit of time because we have a lot of predictor columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Split the data into train/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_combined[features], df_combined[target], test_size=0.33)\n",
    "\n",
    "# Create linear regression object\n",
    "regr = linear_model.LinearRegression()\n",
    "\n",
    "# Train the model using the training sets\n",
    "regr.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions using the testing set\n",
    "y_pred = regr.predict(X_test)\n",
    "\n",
    "# Explained variance score: 1 is perfect prediction\n",
    "print('Train R2 score: %.2f' % r2_score(y_train, regr.predict(X_train)))\n",
    "print('Test R2 score: %.2f' % r2_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Question 8\n",
    "Plot the residual plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# YOUR CODE\n",
    "# Plot residual plot: residuals vs. fitted values\n",
    "# Residuals = y_true - y_pred\n",
    "# Fitted values = y_pred\n",
    "# Don't forget to label your plot\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Question 9\n",
    "Can you reason about the residual plot?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Your Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tree Based Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It has been well established that bagging and other resampling techniques can be used to reduce the variance in model predictions. As several replicates of the original data set are created using random selection with replacement, at every step, each derivative data set is then used to construct a new model and the models are gathered together into an ensemble. To make a prediction, all of the models in the ensemble are polled and their results are averaged in the case of regression, which is our specific approach.\n",
    "\n",
    "As well, it has been well established that a powerful modeling algorithm that makes good use of bagging is Random Forests, which works by training numerous decision trees each based on a different resampling of the original training data. The random forest algorithm improves on bagging by training each tree on a random sample of the available features, to prevent each tree from choosing the same predictors.\n",
    "\n",
    "In Random Forests the bias of the full model is equivalent to the bias of a single decision tree, which itself has high variance. By creating many of these trees, a forest, and then averaging them, the variance of the final model can be greatly reduced over that of a single tree. In practice the only limitation we encountered on the size of the forest is computing time, as an infinite number of trees could be trained without ever increasing bias and with a continual - if asymptotically declining - decrease in the variance.\n",
    "\n",
    "It is for the aforementioned that we considered Random Forests as a Baseline Ensemble and its constituent, the Decision Tree Regressor as a baseline model as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision trees were implemented as to evaluate the base consituent of the Random Forest ensemble. As per our research on previous models, Random Forest is a very good performance candidate for a baseline ensemble model for Rossmann.\n",
    "\n",
    "Decision Trees are also evaluated as a non-parametric baseline model, which enriches the comparative analysis of the Linear Regression Models.\n",
    "\n",
    "For fitting the decision tree model, we do not need to create dummy variables for categorical columns, since trees are able to make use of the factorized category values we created during data cleaning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we considered Random Forest models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rf = RandomForestRegressor(random_state=10)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "print('R2 score on train: ', rf.score(X_train, y_train))\n",
    "print('R2 score on test: ', rf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Question 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the residual plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# YOUR CODE\n",
    "# Plot residual plot: residuals vs. fitted values\n",
    "# Residuals = y_true - y_pred\n",
    "# Fitted values = y_pred\n",
    "# Don't forget to label your plot\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Question 11\n",
    "#### Which model is better? Give 1 possible approach to improve your best model. ** Your answer: **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export models "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our final step, we will choose the best model and explore some important prediction elements. Our app accepts features and gives predictions based on users inputs. Let us see what kind of data input is neccessarry (this is important for you to setup the proper input on the Flask side). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, take one simple observation (we are using first data row):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Print first row\n",
    "X_test.iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict based on your first row:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Test for your app\n",
    "rf.predict(np.array(X_test.iloc[0].reshape(1, -1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore, your input needs to be **numpy array** with the same column order as df_combined. Now, we can export pur model and get ready for the second part of the homework. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.externals import joblib\n",
    "joblib.dump(rf, 'rm.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
